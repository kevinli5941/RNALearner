/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Batch Size (per node): Batch Size (per node): Batch Size (per node):  Batch Size (per node):  3 3 
3
3Learning Rate: 
Learning Rate: 
 Learning Rate:  Learning Rate:  0.010.01 0.01

0.01

Traceback (most recent call last):
Traceback (most recent call last):
  File "/panfs/ccds02/nobackup/people/kli3/RNALearner/pretrain.py", line 73, in <module>
Traceback (most recent call last):
  File "/panfs/ccds02/nobackup/people/kli3/RNALearner/pretrain.py", line 73, in <module>
  File "/panfs/ccds02/nobackup/people/kli3/RNALearner/pretrain.py", line 73, in <module>
Traceback (most recent call last):
  File "/panfs/ccds02/nobackup/people/kli3/RNALearner/pretrain.py", line 73, in <module>
    dist.init_process_group(backend='nccl')        
dist.init_process_group(backend='nccl')dist.init_process_group(backend='nccl')  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group


  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
    dist.init_process_group(backend='nccl')
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 602, in init_process_group
            default_pg = _new_process_group_helper(default_pg = _new_process_group_helper(default_pg = _new_process_group_helper(


  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 727, in _new_process_group_helper
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 727, in _new_process_group_helper
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 727, in _new_process_group_helper
    raise RuntimeError("Distributed package doesn't have NCCL " "built in")
RuntimeError: Distributed package doesn't have NCCL built in
    raise RuntimeError("Distributed package doesn't have NCCL " "built in")
    RuntimeErrorraise RuntimeError("Distributed package doesn't have NCCL " "built in"): 
Distributed package doesn't have NCCL built inRuntimeError
: Distributed package doesn't have NCCL built in
    default_pg = _new_process_group_helper(
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 727, in _new_process_group_helper
    raise RuntimeError("Distributed package doesn't have NCCL " "built in")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 755658) of binary: /home/kli3/.conda/envs/scbert/bin/python3
Traceback (most recent call last):
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-03-30_17:41:14
  host      : gpu001.atstor.adapt.nccs.nasa.gov
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 755659)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-03-30_17:41:14
  host      : gpu001.atstor.adapt.nccs.nasa.gov
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 755660)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-03-30_17:41:14
  host      : gpu001.atstor.adapt.nccs.nasa.gov
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 755661)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-03-30_17:41:14
  host      : gpu001.atstor.adapt.nccs.nasa.gov
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 755658)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
JobID        JobIDRaw        JobName  Partition  MaxVMSize  MaxVMSizeNode  MaxVMSizeTask  AveVMSize     MaxRSS MaxRSSNode MaxRSSTask     AveRSS MaxPages MaxPagesNode   MaxPagesTask   AvePages     MinCPU MinCPUNode MinCPUTask     AveCPU   NTasks  AllocCPUS    Elapsed      State ExitCode AveCPUFreq ReqCPUFreqMin ReqCPUFreqMax ReqCPUFreqGov     ReqMem ConsumedEnergy  MaxDiskRead MaxDiskReadNode MaxDiskReadTask    AveDiskRead MaxDiskWrite MaxDiskWriteNode MaxDiskWriteTask   AveDiskWrite    ReqTRES  AllocTRES TRESUsageInAve TRESUsageInMax TRESUsageInMaxNode TRESUsageInMaxTask TRESUsageInMin TRESUsageInMinNode TRESUsageInMinTask TRESUsageInTot TRESUsageOutMax TRESUsageOutMaxNode TRESUsageOutMaxTask TRESUsageOutAve TRESUsageOutTot 
------------ ------------ ---------- ---------- ---------- -------------- -------------- ---------- ---------- ---------- ---------- ---------- -------- ------------ -------------- ---------- ---------- ---------- ---------- ---------- -------- ---------- ---------- ---------- -------- ---------- ------------- ------------- ------------- ---------- -------------- ------------ --------------- --------------- -------------- ------------ ---------------- ---------------- -------------- ---------- ---------- -------------- -------------- ------------------ ------------------ -------------- ------------------ ------------------ -------------- --------------- ------------------- ------------------- --------------- --------------- 
36514380     36514380     scbertpre+    compute                                                                                                                                                                                                              32   00:00:40    RUNNING      0:0                  Unknown       Unknown       Unknown       720G                                                                                                                                          billing=3+ billing=3+                                                                                                                                                                                                                                 
36514380.ba+ 36514380.ba+      batch                                                                                                                                                                                                               1         32   00:00:40    RUNNING      0:0          0             0             0             0                         0                                                                                                                                      cpu=32,gr+                                                                                                                                                                                                                                 
