/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Batch Size (per node):  3
Learning Rate:  0.0001
Batch Size (per node):  3
Learning Rate:  0.0001
Batch Size (per node):  3
Learning Rate:  0.0001
Batch Size (per node):  3
Learning Rate:  0.0001
DEVICES:  [<torch.cuda.device object at 0x14a398b88550>, <torch.cuda.device object at 0x14a398b88520>, <torch.cuda.device object at 0x14a398b883a0>, <torch.cuda.device object at 0x14a398b88580>]
DEVICES:  [<torch.cuda.device object at 0x152d166bd4c0>, <torch.cuda.device object at 0x152d166bd550>, <torch.cuda.device object at 0x152d166bd3d0>, <torch.cuda.device object at 0x152d166bd5b0>]
DEVICES:  [<torch.cuda.device object at 0x14f4675de4c0>, <torch.cuda.device object at 0x14f4675de550>, <torch.cuda.device object at 0x14f4675de3d0>, <torch.cuda.device object at 0x14f4675de5b0>]
DEVICES:  [<torch.cuda.device object at 0x146b34f3e4c0>, <torch.cuda.device object at 0x146b34f3e550>, <torch.cuda.device object at 0x146b34f3e3d0>, <torch.cuda.device object at 0x146b34f3e5b0>]
/panfs/ccds02/nobackup/people/kli3/RNALearner/performer_pytorch/performer_pytorch.py:115: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /opt/conda/conda-bld/pytorch_1678402374358/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2425.)
  q, r = torch.qr(unstructured_block.cpu(), some = True)
/panfs/ccds02/nobackup/people/kli3/RNALearner/performer_pytorch/performer_pytorch.py:115: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /opt/conda/conda-bld/pytorch_1678402374358/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2425.)
  q, r = torch.qr(unstructured_block.cpu(), some = True)
/panfs/ccds02/nobackup/people/kli3/RNALearner/performer_pytorch/performer_pytorch.py:115: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /opt/conda/conda-bld/pytorch_1678402374358/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2425.)
  q, r = torch.qr(unstructured_block.cpu(), some = True)
/panfs/ccds02/nobackup/people/kli3/RNALearner/performer_pytorch/performer_pytorch.py:115: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /opt/conda/conda-bld/pytorch_1678402374358/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2425.)
  q, r = torch.qr(unstructured_block.cpu(), some = True)
    ==  Epoch: 1 | Training Loss: 0.039225 | Accuracy: 2.1219%  ==
    ==  Epoch: 1 | Validation Loss: 2.221022 | Accuracy: 4.8649%  ==
    ==  Epoch: 2 | Training Loss: 0.017805 | Accuracy: 71.3867%  ==
    ==  Epoch: 2 | Validation Loss: 0.762959 | Accuracy: 73.9865%  ==
    ==  Epoch: 3 | Training Loss: 0.011665 | Accuracy: 79.3147%  ==
    ==  Epoch: 3 | Validation Loss: 0.677459 | Accuracy: 80.5054%  ==
    ==  Epoch: 4 | Training Loss: 0.010776 | Accuracy: 79.3233%  ==
    ==  Epoch: 4 | Validation Loss: 0.623508 | Accuracy: 82.7586%  ==
    ==  Epoch: 5 | Training Loss: 0.010414 | Accuracy: 80.7462%  ==
    ==  Epoch: 5 | Validation Loss: 0.619195 | Accuracy: 71.8121%  ==
    ==  Epoch: 6 | Training Loss: 0.010186 | Accuracy: 80.9030%  ==
    ==  Epoch: 6 | Validation Loss: 0.601472 | Accuracy: 81.8868%  ==
    ==  Epoch: 7 | Training Loss: 0.010059 | Accuracy: 80.8943%  ==
    ==  Epoch: 7 | Validation Loss: 0.594063 | Accuracy: 81.0289%  ==
    ==  Epoch: 8 | Training Loss: 0.009952 | Accuracy: 80.9543%  ==
    ==  Epoch: 8 | Validation Loss: 0.592933 | Accuracy: 84.2105%  ==
    ==  Epoch: 9 | Training Loss: 0.009888 | Accuracy: 81.2007%  ==
    ==  Epoch: 9 | Validation Loss: 0.602988 | Accuracy: 81.3492%  ==
    ==  Epoch: 10 | Training Loss: 0.009814 | Accuracy: 81.3614%  ==
    ==  Epoch: 10 | Validation Loss: 0.586816 | Accuracy: 86.7188%  ==
    ==  Epoch: 11 | Training Loss: 0.009853 | Accuracy: 81.1916%  ==
    ==  Epoch: 11 | Validation Loss: 0.589359 | Accuracy: 79.8701%  ==
    ==  Epoch: 12 | Training Loss: 0.009792 | Accuracy: 81.2523%  ==
    ==  Epoch: 12 | Validation Loss: 0.588062 | Accuracy: 80.7882%  ==
    ==  Epoch: 13 | Training Loss: 0.009777 | Accuracy: 81.2585%  ==
    ==  Epoch: 13 | Validation Loss: 0.586412 | Accuracy: 77.6515%  ==
    ==  Epoch: 14 | Training Loss: 0.009756 | Accuracy: 81.2675%  ==
    ==  Epoch: 14 | Validation Loss: 0.579228 | Accuracy: 81.7241%  ==
    ==  Epoch: 15 | Training Loss: 0.009759 | Accuracy: 81.2877%  ==
    ==  Epoch: 15 | Validation Loss: 0.590392 | Accuracy: 82.7362%  ==
    ==  Epoch: 16 | Training Loss: 0.009740 | Accuracy: 81.2806%  ==
    ==  Epoch: 16 | Validation Loss: 0.581171 | Accuracy: 82.9630%  ==
    ==  Epoch: 17 | Training Loss: 0.009731 | Accuracy: 81.2802%  ==
    ==  Epoch: 17 | Validation Loss: 0.577392 | Accuracy: 85.1429%  ==
    ==  Epoch: 18 | Training Loss: 0.009732 | Accuracy: 81.2409%  ==
    ==  Epoch: 18 | Validation Loss: 0.578333 | Accuracy: 82.5503%  ==
    ==  Epoch: 19 | Training Loss: 0.009687 | Accuracy: 81.3044%  ==
    ==  Epoch: 19 | Validation Loss: 0.581325 | Accuracy: 82.2727%  ==
    ==  Epoch: 20 | Training Loss: 0.009603 | Accuracy: 81.3599%  ==
    ==  Epoch: 20 | Validation Loss: 0.580035 | Accuracy: 78.9474%  ==
    ==  Epoch: 21 | Training Loss: 0.009516 | Accuracy: 81.3388%  ==
    ==  Epoch: 21 | Validation Loss: 0.568778 | Accuracy: 83.3333%  ==
    ==  Epoch: 22 | Training Loss: 0.009422 | Accuracy: 81.3407%  ==
    ==  Epoch: 22 | Validation Loss: 0.562572 | Accuracy: 82.5000%  ==
    ==  Epoch: 23 | Training Loss: 0.009329 | Accuracy: 81.2915%  ==
    ==  Epoch: 23 | Validation Loss: 0.557238 | Accuracy: 82.7815%  ==
    ==  Epoch: 24 | Training Loss: 0.010406 | Accuracy: 80.3215%  ==
    ==  Epoch: 24 | Validation Loss: 0.593610 | Accuracy: 78.4906%  ==
    ==  Epoch: 25 | Training Loss: 0.009624 | Accuracy: 81.2866%  ==
    ==  Epoch: 25 | Validation Loss: 0.571051 | Accuracy: 82.8685%  ==
    ==  Epoch: 26 | Training Loss: 0.009504 | Accuracy: 81.3624%  ==
    ==  Epoch: 26 | Validation Loss: 0.564899 | Accuracy: 83.6120%  ==
    ==  Epoch: 27 | Training Loss: 0.009445 | Accuracy: 81.3771%  ==
    ==  Epoch: 27 | Validation Loss: 0.560697 | Accuracy: 85.9532%  ==
    ==  Epoch: 28 | Training Loss: 0.009374 | Accuracy: 81.3821%  ==
    ==  Epoch: 28 | Validation Loss: 0.562723 | Accuracy: 82.0000%  ==
[E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1578, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802891 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1578, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802892 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1578, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802892 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'std::runtime_error'
terminate called after throwing an instance of '  what():  std::runtime_error[Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1578, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802891 milliseconds before timing out.terminate called after throwing an instance of ''

std::runtime_error'
  what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1578, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802892 milliseconds before timing out.
  what():  [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1578, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1802892 milliseconds before timing out.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 1222030) of binary: /home/kli3/.conda/envs/scbert/bin/python3
Traceback (most recent call last):
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/kli3/.conda/envs/scbert/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
scout_pretrain.py FAILED
--------------------------------------------------------
Failures:
[1]:
  time      : 2023-04-02_06:28:58
  host      : gpu011.atstor.adapt.nccs.nasa.gov
  rank      : 2 (local_rank: 2)
  exitcode  : -6 (pid: 1222031)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1222031
[2]:
  time      : 2023-04-02_06:28:58
  host      : gpu011.atstor.adapt.nccs.nasa.gov
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 1222032)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1222032
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-04-02_06:28:58
  host      : gpu011.atstor.adapt.nccs.nasa.gov
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 1222030)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1222030
========================================================
JobID        JobIDRaw        JobName  Partition  MaxVMSize  MaxVMSizeNode  MaxVMSizeTask  AveVMSize     MaxRSS MaxRSSNode MaxRSSTask     AveRSS MaxPages MaxPagesNode   MaxPagesTask   AvePages     MinCPU MinCPUNode MinCPUTask     AveCPU   NTasks  AllocCPUS    Elapsed      State ExitCode AveCPUFreq ReqCPUFreqMin ReqCPUFreqMax ReqCPUFreqGov     ReqMem ConsumedEnergy  MaxDiskRead MaxDiskReadNode MaxDiskReadTask    AveDiskRead MaxDiskWrite MaxDiskWriteNode MaxDiskWriteTask   AveDiskWrite    ReqTRES  AllocTRES TRESUsageInAve TRESUsageInMax TRESUsageInMaxNode TRESUsageInMaxTask TRESUsageInMin TRESUsageInMinNode TRESUsageInMinTask TRESUsageInTot TRESUsageOutMax TRESUsageOutMaxNode TRESUsageOutMaxTask TRESUsageOutAve TRESUsageOutTot 
------------ ------------ ---------- ---------- ---------- -------------- -------------- ---------- ---------- ---------- ---------- ---------- -------- ------------ -------------- ---------- ---------- ---------- ---------- ---------- -------- ---------- ---------- ---------- -------- ---------- ------------- ------------- ------------- ---------- -------------- ------------ --------------- --------------- -------------- ------------ ---------------- ---------------- -------------- ---------- ---------- -------------- -------------- ------------------ ------------------ -------------- ------------------ ------------------ -------------- --------------- ------------------- ------------------- --------------- --------------- 
36514493     36514493      rc3scout2    compute                                                                                                                                                                                                              32   04:18:48    RUNNING      0:0                  Unknown       Unknown       Unknown       720G                                                                                                                                          billing=3+ billing=3+                                                                                                                                                                                                                                 
36514493.ba+ 36514493.ba+      batch                                                                                                                                                                                                               1         32   04:18:48    RUNNING      0:0          0             0             0             0                         0                                                                                                                                      cpu=32,gr+                                                                                                                                                                                                                                 
